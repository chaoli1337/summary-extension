// Background service worker for managing tabs and API calls
import { SummaryCache } from './cache';
import { claudeAPI } from './api-integrations/claude';
import { openRouterAPI } from './api-integrations/openrouter';
import { portkeyAPI } from './api-integrations/portkey';
import { 
  LLMResponse, 
  ChatMessage, 
  Language, 
  CustomPrompts,
  ModelProvider
} from './api-integrations/types';

// Custom fetch adapter for Chrome extension service worker context
const customFetch = async (url: string, options: RequestInit): Promise<Response> => {
  return fetch(url, {
    ...options,
    // Ensure we're using the global fetch properly
    mode: 'cors',
    credentials: 'omit'
  });
};


interface TabInfo {
  id: number;
  url: string;
  title: string;
  text?: string;
}

interface LLMRequest {
  text: string;
  model: ModelProvider;
  modelIdentifier?: string;
  apiKey: string;
  apiUrl?: string;
  virtualKey?: string;
  language?: Language;
  customPrompts?: CustomPrompts;
}

interface ChatRequest {
  messages: ChatMessage[];
  model: ModelProvider;
  modelIdentifier?: string;
  apiKey: string;
  apiUrl?: string;
  virtualKey?: string;
  language?: Language;
  customPrompts?: CustomPrompts;
}

// LLMResponse is now imported from types.ts

interface PendingRequest {
  id: string;
  tabUrl: string;
  status: 'pending' | 'processing' | 'completed' | 'error';
  result?: LLMResponse;
  error?: string;
  timestamp: number;
}

class BackgroundService {
  private static instance: BackgroundService;
  private pendingRequests = new Map<string, PendingRequest>();
  private cleanupInterval: any = null;
  private conversationContexts = new Map<string, ChatMessage[]>();

  static getInstance(): BackgroundService {
    if (!this.instance) {
      this.instance = new BackgroundService();
      this.instance.startCleanupTimer();
    }
    return this.instance;
  }

  private startCleanupTimer() {
    // Clean up old requests every 5 minutes
    this.cleanupInterval = setInterval(() => {
      this.cleanupOldRequests();
    }, 5 * 60 * 1000);
  }


  private cleanupOldRequests() {
    const now = Date.now();
    const maxAge = 30 * 60 * 1000; // 30 minutes
    
    for (const [id, request] of this.pendingRequests) {
      if (now - request.timestamp > maxAge) {
        this.pendingRequests.delete(id);
      }
    }
  }

  async getAllTabsInfo(): Promise<TabInfo[]> {
    try {
      const tabs = await chrome.tabs.query({});
      return tabs.map(tab => ({
        id: tab.id!,
        url: tab.url || '',
        title: tab.title || 'Untitled'
      }));
    } catch (error) {
      console.error('Error getting tabs info:', error);
      return [];
    }
  }

  async extractTextFromTab(tabId: number): Promise<string> {
    try {
      const results = await chrome.scripting.executeScript({
        target: { tabId },
        func: () => {
          // Extract text content from the page
          const walker = document.createTreeWalker(
            document.body,
            NodeFilter.SHOW_TEXT,
            {
              acceptNode: (node) => {
                const parent = node.parentElement;
                if (!parent) return NodeFilter.FILTER_REJECT;

                const style = window.getComputedStyle(parent);
                if (style.display === 'none' || style.visibility === 'hidden') {
                  return NodeFilter.FILTER_REJECT;
                }

                const ignoredTags = new Set(['SCRIPT', 'STYLE', 'NOSCRIPT', 'IFRAME']);
                if (ignoredTags.has(parent.tagName)) {
                  return NodeFilter.FILTER_REJECT;
                }

                return NodeFilter.FILTER_ACCEPT;
              }
            }
          );

          const textNodes: string[] = [];
          let node;

          while (node = walker.nextNode()) {
            const text = node.textContent?.trim();
            if (text && text.length > 0) {
              textNodes.push(text);
            }
          }

          return textNodes.join(' ').replace(/\s+/g, ' ').trim();
        }
      });

      return results[0]?.result || '';
    } catch (error) {
      console.error('Error extracting text from tab:', error);
      return '';
    }
  }

  async startLLMRequest(request: LLMRequest & { url?: string; forceFresh?: boolean }, requestId?: string): Promise<string> {
    const id = requestId || this.generateRequestId();
    const pendingRequest: PendingRequest = {
      id,
      tabUrl: request.url || '',
      status: 'pending',
      timestamp: Date.now()
    };
    
    this.pendingRequests.set(id, pendingRequest);
    
    // Process request asynchronously
    this.processLLMRequest(id, request).catch(error => {
      console.error('LLM request processing failed:', error);
      const req = this.pendingRequests.get(id);
      if (req) {
        req.status = 'error';
        req.error = error instanceof Error ? error.message : 'Unknown error';
        this.pendingRequests.set(id, req);
      }
    });
    
    return id;
  }

  async processLLMRequest(requestId: string, request: LLMRequest & { url?: string; forceFresh?: boolean }): Promise<void> {
    const pendingRequest = this.pendingRequests.get(requestId);
    if (!pendingRequest) return;
    
    pendingRequest.status = 'processing';
    this.pendingRequests.set(requestId, pendingRequest);
    
    try {
      const result = await this.callLLMAPI(request);
      pendingRequest.status = 'completed';
      pendingRequest.result = result;
      this.pendingRequests.set(requestId, pendingRequest);
    } catch (error) {
      pendingRequest.status = 'error';
      pendingRequest.error = error instanceof Error ? error.message : 'Unknown error';
      this.pendingRequests.set(requestId, pendingRequest);
    }
  }

  getRequestStatus(requestId: string): PendingRequest | null {
    return this.pendingRequests.get(requestId) || null;
  }

  private generateRequestId(): string {
    return `req_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  async callLLMAPI(request: LLMRequest & { url?: string; forceFresh?: boolean }): Promise<LLMResponse> {
    try {
      const { text, model, apiKey, apiUrl, url, language = 'chinese', forceFresh = false } = request;


      // Check cache first if URL is provided and not forcing fresh
      if (url && !forceFresh) {
        const cachedSummary = await SummaryCache.get(url, language);
        if (cachedSummary) {
          // Get cache entry details for timestamp
          const cacheEntry = await this.getCacheEntry(url, language);
          return { 
            summary: cachedSummary,
            fromCache: true,
            cachedAt: cacheEntry?.timestamp
          };
        }
      } else if (forceFresh) {
      } else {
      }

      // Cache miss or no URL provided, make API call
      let apiResponse: LLMResponse;

      if (model === 'claude') {
        apiResponse = await claudeAPI.callAPI(text, apiKey, language, request.customPrompts, request.modelIdentifier);
      } else if (model === 'portkey') {
        apiResponse = await portkeyAPI.callAPI(text, apiKey, apiUrl, request.virtualKey, language, request.customPrompts, request.modelIdentifier);
      } else {
        apiResponse = await openRouterAPI.callAPI(text, model, apiKey, apiUrl, language, request.customPrompts, request.modelIdentifier);
      }

      // Cache the response if successful and URL is provided
      if (url && apiResponse.summary && !apiResponse.error) {
        await SummaryCache.set(url, language, apiResponse.summary, model);
      } else {
      }

      return apiResponse;
    } catch (error) {
      console.error('LLM API call failed:', error);
      return {
        summary: '',
        error: error instanceof Error ? error.message : 'Unknown error'
      };
    }
  }

  private async getCacheEntry(url: string, language: string): Promise<any> {
    try {
      const result = await chrome.storage.local.get(['summary_cache']);
      const cache = result['summary_cache'] || {};
      const cacheKey = `${url}|${language}`;
      return cache[cacheKey] || null;
    } catch (error) {
      console.error('Error getting cache entry:', error);
      return null;
    }
  }


    const endpoint = apiUrl || 'https://openrouter.ai/api/v1/chat/completions';

    // Get prompt configuration
    const promptConfig = this.getPromptConfig(language, customPrompts);
    
    // Use provided model identifier or default mapping
    let modelToUse: string;
    if (modelIdentifier) {
      // If custom model identifier is provided, use it with provider prefix
      modelToUse = model === 'claude' ? `anthropic/${modelIdentifier}` : `openai/${modelIdentifier}`;
    } else {
      // Use default mapping
      const modelMap = {
        'claude': 'anthropic/claude-sonnet-4-20250514',
        'openai': 'openai/gpt-4'
      };
      modelToUse = modelMap[model];
    }

    try {
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`,
          'Content-Type': 'application/json',
          'HTTP-Referer': chrome.runtime.getURL(''),
          'X-Title': 'AI Page Summarizer'
        },
        body: JSON.stringify({
          model: modelToUse,
          messages: [
            {
              role: 'system',
              content: promptConfig.systemPrompt
            },
            {
              role: 'user',
              content: this.createPrompt(text, language, customPrompts)
            }
          ],
          max_tokens: promptConfig.maxTokens,
          temperature: promptConfig.temperature
        })
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        const errorDetails = errorData.error?.message || errorData.message || 'Unknown error';
        const statusCode = response.status;
        const statusText = response.statusText;

        // Provide more specific error messages
        let enhancedError = `API request failed (${statusCode} ${statusText}): ${errorDetails}`;

        if (statusCode === 401) {
          enhancedError += '\n\nTip: Check if your API key is valid and active.';
        } else if (statusCode === 429) {
          enhancedError += '\n\nTip: You have hit rate limits. Please wait before trying again.';
        } else if (statusCode >= 500) {
          enhancedError += '\n\nTip: This is a server error. The API service may be temporarily unavailable.';
        }

        throw new Error(enhancedError);
      }

      const data = await response.json();
      const summary = data.choices?.[0]?.message?.content || 'No summary generated';
      return { summary };
    } catch (error) {
      if (error instanceof TypeError && error.message.includes('fetch')) {
        throw new Error(`Network error: Unable to connect to API endpoint. Check your internet connection and URL: ${endpoint}`);
      }
      throw error;
    }
  }

    try {
      // Try SDK approach first
      try {
        const portkeyConfig: any = {
          apiKey: apiKey
        };

        // Add virtual key if provided
        if (virtualKey) {
          portkeyConfig.virtualKey = virtualKey;
        }

        // Add custom base URL if provided
        if (apiUrl) {
          portkeyConfig.baseURL = apiUrl;
        }

        const portkey = new Portkey(portkeyConfig);

        // Get prompt configuration
        const promptConfig = this.getPromptConfig(language, customPrompts);
        const model = this.getModelIdentifier('portkey', modelIdentifier);

        // Create chat completion using SDK
        const response = await portkey.chat.completions.create({
          messages: [
            {
              role: 'system',
              content: promptConfig.systemPrompt
            },
            {
              role: 'user',
              content: this.createPrompt(text, language, customPrompts)
            }
          ],
          model: model, // Use configurable model identifier
          max_tokens: promptConfig.maxTokens,
          temperature: promptConfig.temperature
        });

        const summary = response.choices?.[0]?.message?.content?.toString() || 'No summary generated';
        return { summary };
      } catch (sdkError: any) {
        // If SDK fails, fall back to direct API call
        return await this.callPortkeyDirectAPI(text, apiKey, apiUrl, virtualKey, language, customPrompts, modelIdentifier);
      }
    } catch (error: any) {
      // Handle SDK-specific errors
      let errorMessage = 'Portkey API request failed';
      
      if (error?.status) {
        const statusCode = error.status;
        const errorDetails = error.message || error.error?.message || 'Unknown error';
        
        errorMessage = `Portkey API request failed (${statusCode}): ${errorDetails}`;

        if (statusCode === 401) {
          errorMessage += '\n\nTip: Check if your Portkey API key is valid and active.';
        } else if (statusCode === 403) {
          errorMessage += '\n\nTip: Check if your virtual key is valid and has the required permissions.';
        } else if (statusCode === 429) {
          errorMessage += '\n\nTip: You have hit rate limits. Please wait before trying again.';
        } else if (statusCode >= 500) {
          errorMessage += '\n\nTip: This is a server error. The Portkey service may be temporarily unavailable.';
        }
      } else if (error?.message) {
        errorMessage += `: ${error.message}`;
      }

      throw new Error(errorMessage);
    }
  }

    const endpoint = apiUrl || 'https://api.portkey.ai/v1/chat/completions';

    try {
      const headers: Record<string, string> = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
        'X-Title': 'AI Page Summarizer'
      };

      // Add virtual key if provided
      if (virtualKey) {
        headers['x-portkey-virtual-key'] = virtualKey;
      }

      // Get prompt configuration
      const promptConfig = this.getPromptConfig(language, customPrompts);
      const model = this.getModelIdentifier('portkey', modelIdentifier);

      const response = await fetch(endpoint, {
        method: 'POST',
        headers,
        body: JSON.stringify({
          model: model, // Use configurable model identifier
          messages: [
            {
              role: 'system',
              content: promptConfig.systemPrompt
            },
            {
              role: 'user',
              content: this.createPrompt(text, language, customPrompts)
            }
          ],
          max_tokens: promptConfig.maxTokens,
          temperature: promptConfig.temperature
        })
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        const errorDetails = errorData.error?.message || errorData.message || 'Unknown error';
        const statusCode = response.status;
        const statusText = response.statusText;

        // Provide more specific error messages
        let enhancedError = `Portkey API request failed (${statusCode} ${statusText}): ${errorDetails}`;

        if (statusCode === 401) {
          enhancedError += '\n\nTip: Check if your Portkey API key is valid and active.';
        } else if (statusCode === 403) {
          enhancedError += '\n\nTip: Check if your virtual key is valid and has the required permissions.';
        } else if (statusCode === 429) {
          enhancedError += '\n\nTip: You have hit rate limits. Please wait before trying again.';
        } else if (statusCode >= 500) {
          enhancedError += '\n\nTip: This is a server error. The Portkey service may be temporarily unavailable.';
        }

        throw new Error(enhancedError);
      }

      const data = await response.json();
      const summary = data.choices?.[0]?.message?.content || 'No summary generated';
      return { summary };
    } catch (error) {
      if (error instanceof TypeError && error.message.includes('fetch')) {
        throw new Error(`Network error: Unable to connect to Portkey API endpoint. Check your internet connection and URL: ${endpoint}`);
      }
      throw error;
    }
  }

  async openDetachedWindow(): Promise<void> {
    try {
      // Check if detached popup window already exists
      const existingWindows = await chrome.windows.getAll({ populate: true });
      const popupWindow = existingWindows.find(window =>
        window.type === 'popup' &&
        window.tabs?.some(tab => tab.url?.includes('popup.html'))
      );

      if (popupWindow) {
        // Focus existing popup window
        await chrome.windows.update(popupWindow.id!, { focused: true });
      } else {
        // Create new popup window
        await chrome.windows.create({
          url: chrome.runtime.getURL('popup.html'),
          type: 'popup',
          width: 400,
          height: 500,
          focused: true
        });
      }
    } catch (error) {
      console.error('Error creating detached popup window:', error);
      throw error;
    }
  }

    const defaultConfig = {
      chinese: {
        systemPrompt: '你是一个有用的助手，专门总结网页内容。请提供简洁、结构清晰的摘要，突出主要观点和关键信息。',
        userPrompt: '请为以下网页内容提供一个简洁、结构清晰的中文摘要，突出主要观点和关键信息：\n\n{text}',
        temperature: 0.5,
        maxTokens: 1000
      },
      english: {
        systemPrompt: 'You are a helpful assistant that summarizes web page content. Provide a concise, well-structured summary highlighting the main points and key information.',
        userPrompt: 'Please provide a concise, well-structured summary of this webpage content, highlighting the main points and key information:\n\n{text}',
        temperature: 0.5,
        maxTokens: 1000
      }
    };

    if (customPrompts && customPrompts[language]) {
      return customPrompts[language];
    }

    return defaultConfig[language];
  }

    const promptConfig = this.getPromptConfig(language, customPrompts);
    return promptConfig.userPrompt.replace('{text}', text);
  }

  async getCacheStats() {
    return await SummaryCache.getStats();
  }

  async clearCache() {
    return await SummaryCache.clear();
  }

  async callChatAPI(request: ChatRequest & { tabId?: number }): Promise<LLMResponse> {
    try {
      const { messages, model, apiKey, apiUrl, virtualKey, language = 'chinese', customPrompts, tabId } = request;


      // Store conversation context for this tab if provided
      if (tabId !== undefined) {
        this.conversationContexts.set(`tab_${tabId}`, messages);
      }

      let apiResponse: LLMResponse;

      if (model === 'claude') {
        apiResponse = await claudeAPI.callChatAPI(messages, apiKey, language, customPrompts, request.modelIdentifier);
      } else if (model === 'portkey') {
        // Check if we need extremely large context handling
        const totalLength = messages.reduce((sum, msg) => sum + msg.content.length, 0);
        const estimatedTokens = Math.ceil(totalLength / 4);
        
        if (estimatedTokens > 100000) {
          // For extremely large contexts, we still need special handling
          apiResponse = await this.handleExtremelyLargeContext(messages, apiKey, apiUrl, virtualKey, language, customPrompts, request.modelIdentifier);
        } else {
          apiResponse = await portkeyAPI.callLargeContextAPI(messages, apiKey, apiUrl, virtualKey, language, customPrompts, request.modelIdentifier);
        }
      } else {
        apiResponse = await openRouterAPI.callChatAPI(messages, model, apiKey, apiUrl, language, customPrompts, request.modelIdentifier);
      }

      return apiResponse;
    } catch (error) {
      console.error('Chat API call failed:', error);
      return {
        summary: '',
        error: error instanceof Error ? error.message : 'Unknown error'
      };
    }
  }

    const endpoint = 'https://api.anthropic.com/v1/messages';

    // Get prompt configuration for chat
    const promptConfig = this.getPromptConfig(language, customPrompts);
    const model = this.getModelIdentifier('claude', modelIdentifier);

    try {
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'x-api-key': apiKey,
          'Content-Type': 'application/json',
          'anthropic-version': '2023-06-01',
          'anthropic-dangerous-direct-browser-access': 'true'
        },
        body: JSON.stringify({
          model: model,
          max_tokens: promptConfig.maxTokens,
          messages: messages
        })
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        const errorDetails = errorData.error?.message || errorData.message || 'Unknown error';
        const statusCode = response.status;
        const statusText = response.statusText;

        throw new Error(`Claude API request failed (${statusCode} ${statusText}): ${errorDetails}`);
      }

      const data = await response.json();
      const summary = data.content?.[0]?.text || 'No response generated';
      return { summary };
    } catch (error) {
      if (error instanceof TypeError && error.message.includes('fetch')) {
        throw new Error(`Network error: Unable to connect to Claude API. Check your internet connection.`);
      }
      throw error;
    }
  }

    const endpoint = apiUrl || 'https://openrouter.ai/api/v1/chat/completions';

    // Get prompt configuration for chat
    const promptConfig = this.getPromptConfig(language, customPrompts);
    
    // Use provided model identifier or default mapping
    let modelToUse: string;
    if (modelIdentifier) {
      // If custom model identifier is provided, use it with provider prefix
      modelToUse = model === 'claude' ? `anthropic/${modelIdentifier}` : `openai/${modelIdentifier}`;
    } else {
      // Use default mapping
      const modelMap = {
        'claude': 'anthropic/claude-sonnet-4-20250514',
        'openai': 'openai/gpt-4'
      };
      modelToUse = modelMap[model];
    }

    try {
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`,
          'Content-Type': 'application/json',
          'HTTP-Referer': chrome.runtime.getURL(''),
          'X-Title': 'AI Page Summarizer'
        },
        body: JSON.stringify({
          model: modelToUse,
          messages: messages,
          max_tokens: promptConfig.maxTokens,
          temperature: promptConfig.temperature
        })
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        const errorDetails = errorData.error?.message || errorData.message || 'Unknown error';
        const statusCode = response.status;
        const statusText = response.statusText;

        let enhancedError = `API request failed (${statusCode} ${statusText}): ${errorDetails}`;

        if (statusCode === 401) {
          enhancedError += '\n\nTip: Check if your API key is valid and active.';
        } else if (statusCode === 429) {
          enhancedError += '\n\nTip: You have hit rate limits. Please wait before trying again.';
        } else if (statusCode >= 500) {
          enhancedError += '\n\nTip: This is a server error. The API service may be temporarily unavailable.';
        }

        throw new Error(enhancedError);
      }

      const data = await response.json();
      const summary = data.choices?.[0]?.message?.content || 'No response generated';
      return { summary };
    } catch (error) {
      if (error instanceof TypeError && error.message.includes('fetch')) {
        throw new Error(`Network error: Unable to connect to API endpoint. Check your internet connection and URL: ${endpoint}`);
      }
      throw error;
    }
  }

    try {
      // Try SDK approach first
      try {
        const portkeyConfig: any = {
          apiKey: apiKey
        };

        // Add virtual key if provided
        if (virtualKey) {
          portkeyConfig.virtualKey = virtualKey;
        }

        // Add custom base URL if provided
        if (apiUrl) {
          portkeyConfig.baseURL = apiUrl;
        }

        const portkey = new Portkey(portkeyConfig);

        // Get prompt configuration for chat
        const promptConfig = this.getPromptConfig(language, customPrompts);

        // Create chat completion using SDK
        const response = await portkey.chat.completions.create({
          messages: messages,
          model: 'gpt-4', // Default model, can be overridden by virtual key
          max_tokens: promptConfig.maxTokens,
          temperature: promptConfig.temperature
        });

        const summary = response.choices?.[0]?.message?.content?.toString() || 'No response generated';
        return { summary };
      } catch (sdkError: any) {
        // If SDK fails, fall back to direct API call
        return await this.callPortkeyDirectChatAPI(messages, apiKey, apiUrl, virtualKey, language, customPrompts);
      }
    } catch (error: any) {
      // Handle SDK-specific errors
      let errorMessage = 'Portkey API request failed';
      
      if (error?.status) {
        const statusCode = error.status;
        const errorDetails = error.message || error.error?.message || 'Unknown error';
        
        errorMessage = `Portkey API request failed (${statusCode}): ${errorDetails}`;

        if (statusCode === 401) {
          errorMessage += '\n\nTip: Check if your Portkey API key is valid and active.';
        } else if (statusCode === 403) {
          errorMessage += '\n\nTip: Check if your virtual key is valid and has the required permissions.';
        } else if (statusCode === 429) {
          errorMessage += '\n\nTip: You have hit rate limits. Please wait before trying again.';
        } else if (statusCode >= 500) {
          errorMessage += '\n\nTip: This is a server error. The Portkey service may be temporarily unavailable.';
        }
      } else if (error?.message) {
        errorMessage += `: ${error.message}`;
      }

      throw new Error(errorMessage);
    }
  }

    const endpoint = apiUrl || 'https://api.portkey.ai/v1/chat/completions';

    try {
      const headers: Record<string, string> = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
        'X-Title': 'AI Page Summarizer'
      };

      // Add virtual key if provided
      if (virtualKey) {
        headers['x-portkey-virtual-key'] = virtualKey;
      }

      // Get prompt configuration for chat
      const promptConfig = this.getPromptConfig(language, customPrompts);

      const response = await fetch(endpoint, {
        method: 'POST',
        headers,
        body: JSON.stringify({
          model: 'gpt-4', // Default model, can be overridden by virtual key
          messages: messages,
          max_tokens: promptConfig.maxTokens,
          temperature: promptConfig.temperature
        })
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        const errorDetails = errorData.error?.message || errorData.message || 'Unknown error';
        const statusCode = response.status;
        const statusText = response.statusText;

        let enhancedError = `Portkey API request failed (${statusCode} ${statusText}): ${errorDetails}`;

        if (statusCode === 401) {
          enhancedError += '\n\nTip: Check if your Portkey API key is valid and active.';
        } else if (statusCode === 403) {
          enhancedError += '\n\nTip: Check if your virtual key is valid and has the required permissions.';
        } else if (statusCode === 429) {
          enhancedError += '\n\nTip: You have hit rate limits. Please wait before trying again.';
        } else if (statusCode >= 500) {
          enhancedError += '\n\nTip: This is a server error. The Portkey service may be temporarily unavailable.';
        }

        throw new Error(enhancedError);
      }

      const data = await response.json();
      const summary = data.choices?.[0]?.message?.content || 'No response generated';
      return { summary };
    } catch (error) {
      if (error instanceof TypeError && error.message.includes('fetch')) {
        throw new Error(`Network error: Unable to connect to Portkey API endpoint. Check your internet connection and URL: ${endpoint}`);
      }
      throw error;
    }
  }

  // Add this new method for large context conversations
    messages: Array<{role: 'system' | 'user' | 'assistant'; content: string}>, 
    apiKey: string, 
    apiUrl?: string, 
    virtualKey?: string, 
    language: 'chinese' | 'english' = 'chinese', 
    customPrompts?: any,
    modelIdentifier?: string
  ): Promise<LLMResponse> {
    try {
      // Try SDK approach first
      try {
        const portkeyConfig: any = {
          apiKey: apiKey
        };

        if (virtualKey) {
          portkeyConfig.virtualKey = virtualKey;
        }

        if (apiUrl) {
          portkeyConfig.baseURL = apiUrl;
        }

        const portkey = new Portkey(portkeyConfig);

        // Get prompt configuration
        const promptConfig = this.getPromptConfig(language, customPrompts);

        // For large context, we need to manage token limits
        // Most models have limits, so we'll implement smart truncation
        const processedMessages = this.processLargeContext(messages, promptConfig.maxTokens);

        const response = await portkey.chat.completions.create({
          messages: processedMessages,
          model: 'gpt-4', // or use virtual key to route to specific model
          max_tokens: promptConfig.maxTokens,
          temperature: promptConfig.temperature
        });

        const summary = response.choices?.[0]?.message?.content?.toString() || 'No response generated';
        return { summary };
      } catch (sdkError: any) {
        return await this.callPortkeyDirectLargeContextAPI(messages, apiKey, apiUrl, virtualKey, language, customPrompts);
      }
    } catch (error: any) {
      let errorMessage = 'Portkey large context API request failed';
      
      if (error.message) {
        errorMessage += `: ${error.message}`;
      }
      
      throw new Error(errorMessage);
    }
  }

  // Process large context by implementing smart truncation
    messages: Array<{role: 'system' | 'user' | 'assistant'; content: string}>, 
    maxTokens: number
  ): Array<{role: 'system' | 'user' | 'assistant'; content: string}> {
    
    // Rough token estimation (4 characters per token)
    const estimateTokens = (text: string): number => Math.ceil(text.length / 4);
    
    // Calculate total tokens in current messages
    let totalTokens = 0;
    const processedMessages = [];
    
    // Always keep system message
    if (messages.length > 0 && messages[0].role === 'system') {
      processedMessages.push(messages[0]);
      totalTokens += estimateTokens(messages[0].content);
    }
    
    // Process remaining messages from newest to oldest (keep most recent)
    const remainingMessages = messages.slice(1).reverse();
    
    for (const message of remainingMessages) {
      const messageTokens = estimateTokens(message.content);
      
      // Reserve space for response (roughly 1/3 of max tokens)
      const reservedTokens = Math.floor(maxTokens / 3);
      const availableTokens = maxTokens - reservedTokens;
      
      if (totalTokens + messageTokens <= availableTokens) {
        processedMessages.unshift(message); // Add to beginning (maintain order)
        totalTokens += messageTokens;
      } else {
        // If message is too long, truncate it
        if (messageTokens > availableTokens - totalTokens) {
          const availableChars = (availableTokens - totalTokens) * 4;
          const truncatedContent = message.content.substring(0, availableChars) + '...';
          processedMessages.unshift({
            role: message.role,
            content: truncatedContent
          });
          break;
        }
      }
    }
    
    return processedMessages;
  }

  // Direct API fallback for large context
    messages: Array<{role: 'system' | 'user' | 'assistant'; content: string}>, 
    apiKey: string, 
    apiUrl?: string, 
    virtualKey?: string, 
    language: 'chinese' | 'english' = 'chinese', 
    customPrompts?: any,
    modelIdentifier?: string
  ): Promise<LLMResponse> {
    const endpoint = apiUrl || 'https://api.portkey.ai/v1/chat/completions';
    
    try {
      const promptConfig = this.getPromptConfig(language, customPrompts);
      const processedMessages = this.processLargeContext(messages, promptConfig.maxTokens);
      const model = this.getModelIdentifier('portkey', modelIdentifier);
      
      const headers: Record<string, string> = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      };

      if (virtualKey) {
        headers['x-portkey-virtual-key'] = virtualKey;
      }

      const response = await fetch(endpoint, {
        method: 'POST',
        headers,
        body: JSON.stringify({
          model: model, // Use configurable model identifier
          messages: processedMessages,
          max_tokens: promptConfig.maxTokens,
          temperature: promptConfig.temperature
        })
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        const errorDetails = errorData.error?.message || errorData.message || 'Unknown error';
        throw new Error(`Portkey API request failed (${response.status}): ${errorDetails}`);
      }

      const data = await response.json();
      const summary = data.choices?.[0]?.message?.content || 'No response generated';
      return { summary };
    } catch (error) {
      if (error instanceof TypeError && error.message.includes('fetch')) {
        throw new Error(`Network error: Unable to connect to Portkey API endpoint: ${endpoint}`);
      }
      throw error;
    }
  }

  // Handle extremely large contexts (1M+ tokens) with smart chunking
  private async handleExtremelyLargeContext(
    messages: Array<{role: 'system' | 'user' | 'assistant'; content: string}>, 
    apiKey: string, 
    apiUrl?: string, 
    virtualKey?: string, 
    language: 'chinese' | 'english' = 'chinese', 
    customPrompts?: any,
    modelIdentifier?: string
  ): Promise<LLMResponse> {
    
    // If context is manageable, use regular large context API
    const totalLength = messages.reduce((sum, msg) => sum + msg.content.length, 0);
    const estimatedTokens = Math.ceil(totalLength / 4);
    
    // If under 100K tokens, use regular processing
    if (estimatedTokens < 100000) {
      return await portkeyAPI.callLargeContextAPI(messages, apiKey, apiUrl, virtualKey, language, customPrompts, modelIdentifier);
    }
    
    
    // For extremely large contexts, implement smart chunking
    const chunkedMessages = this.chunkLargeContext(messages);
    
    // Process each chunk and maintain conversation flow
    let processedContext = '';
    let conversationSummary = '';
    
    for (let i = 0; i < chunkedMessages.length; i++) {
      const chunk = chunkedMessages[i];
      
      // Create a summary of previous chunks if we have them
      if (i > 0 && processedContext.length > 0) {
        const summaryPrompt = [
          {
            role: 'system' as const,
            content: language === 'chinese' 
              ? '请总结以下对话内容，保留关键信息和上下文，以便后续对话使用。'
              : 'Please summarize the following conversation content, preserving key information and context for subsequent conversations.'
          },
          {
            role: 'user' as const,
            content: processedContext
          }
        ];
        
        try {
          const summaryResponse = await portkeyAPI.callAPI(
            processedContext, 
            apiKey, 
            apiUrl, 
            virtualKey, 
            language, 
            customPrompts,
            modelIdentifier
          );
          conversationSummary = summaryResponse.summary;
        } catch (error) {
          console.warn('Failed to create conversation summary:', error);
        }
      }
      
      // Process current chunk
      const chunkWithSummary = this.integrateSummaryWithChunk(chunk, conversationSummary, language);
      
      try {
        const response = await portkeyAPI.callLargeContextAPI(
          chunkWithSummary, 
          apiKey, 
          apiUrl, 
          virtualKey, 
          language, 
          customPrompts,
          modelIdentifier
        );
        
        // Update processed context
        processedContext += '\n\n' + chunk.map(msg => `${msg.role}: ${msg.content}`).join('\n');
        
        // If this is the last chunk, return the response
        if (i === chunkedMessages.length - 1) {
          return response;
        }
        
      } catch (error) {
        console.error(`Error processing chunk ${i}:`, error);
        throw error;
      }
    }
    
    throw new Error('Failed to process large context');
  }
  
  // Chunk large context into manageable pieces
  private chunkLargeContext(
    messages: Array<{role: 'system' | 'user' | 'assistant'; content: string}>
  ): Array<Array<{role: 'system' | 'user' | 'assistant'; content: string}>> {
    
    const maxChunkSize = 50000; // ~50K tokens per chunk
    const chunks: Array<Array<{role: 'system' | 'user' | 'assistant'; content: string}>> = [];
    let currentChunk: Array<{role: 'system' | 'user' | 'assistant'; content: string}> = [];
    let currentChunkSize = 0;
    
    for (const message of messages) {
      const messageSize = message.content.length;
      
      // If adding this message would exceed chunk size, start a new chunk
      if (currentChunkSize + messageSize > maxChunkSize && currentChunk.length > 0) {
        chunks.push([...currentChunk]);
        currentChunk = [];
        currentChunkSize = 0;
      }
      
      // Add message to current chunk
      currentChunk.push(message);
      currentChunkSize += messageSize;
    }
    
    // Add the last chunk if it has content
    if (currentChunk.length > 0) {
      chunks.push(currentChunk);
    }
    
    return chunks;
  }
  
  // Integrate conversation summary with current chunk
  private integrateSummaryWithChunk(
    chunk: Array<{role: 'system' | 'user' | 'assistant'; content: string}>,
    summary: string,
    language: 'chinese' | 'english'
  ): Array<{role: 'system' | 'user' | 'assistant'; content: string}> {
    
    if (!summary) {
      return chunk;
    }
    
    const summaryMessage = {
      role: 'system' as const,
      content: language === 'chinese'
        ? `之前的对话摘要：${summary}\n\n请基于这个摘要和当前对话继续回答。`
        : `Previous conversation summary: ${summary}\n\nPlease continue the conversation based on this summary and the current dialogue.`
    };
    
    return [summaryMessage, ...chunk];
  }

  getConversationContext(tabId: number): Array<{role: 'system' | 'user' | 'assistant'; content: string}> | null {
    const key = `tab_${tabId}`;
    return this.conversationContexts.get(key) || null;
  }

  clearConversationContext(tabId: number): void {
    const key = `tab_${tabId}`;
    this.conversationContexts.delete(key);
  }

  clearAllConversationContexts(): void {
    this.conversationContexts.clear();
  }
}

// Message handling
chrome.runtime.onMessage.addListener((request, _sender, sendResponse) => {
  const service = BackgroundService.getInstance();

  switch (request.action) {
    case 'getAllTabs':
      service.getAllTabsInfo().then(sendResponse);
      return true;

    case 'extractTabText':
      service.extractTextFromTab(request.tabId).then(sendResponse);
      return true;

    case 'startSummarize':
      // Start async summarization and return request ID immediately
      const dataWithUrl = { ...request.data, url: request.url, forceFresh: request.forceFresh };
      service.startLLMRequest(dataWithUrl, request.requestId).then(requestId => {
        sendResponse({ requestId });
      }).catch(error => {
        sendResponse({ error: error.message });
      });
      return true;

    case 'getRequestStatus':
      // Check status of a request by ID
      const status = service.getRequestStatus(request.requestId);
      sendResponse(status);
      return true;

    case 'summarizeText':
      // Legacy synchronous method (kept for compatibility)
      const legacyDataWithUrl = { ...request.data, url: request.url, forceFresh: request.forceFresh };
      service.callLLMAPI(legacyDataWithUrl).then(sendResponse);
      return true;

    case 'openDetachedWindow':
      service.openDetachedWindow().then(sendResponse);
      return true;

    case 'getCacheStats':
      service.getCacheStats().then(sendResponse);
      return true;

    case 'clearCache':
      service.clearCache().then(() => sendResponse({ success: true }));
      return true;

    case 'chatMessage':
      // Include tab ID for context tracking
      const chatRequest = { ...request.data, tabId: request.tabId };
      service.callChatAPI(chatRequest).then(sendResponse);
      return true;

    case 'getConversationContext':
      // Get stored conversation context for a tab
      const context = service.getConversationContext(request.tabId);
      sendResponse({ context });
      return true;

    case 'clearConversationContext':
      // Clear conversation context for a tab
      service.clearConversationContext(request.tabId);
      sendResponse({ success: true });
      return true;

    default:
      sendResponse({ error: 'Unknown action' });
  }
});

// The extension now uses default popup behavior defined in manifest.json
// No need for custom action click handler

// Extension installation
chrome.runtime.onInstalled.addListener(() => {
});